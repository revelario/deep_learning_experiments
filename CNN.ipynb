{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import time\n",
    "## cPicklne no worky in Python3\n",
    "#from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function to read in the batch files\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin') # encoding latin otherwise no worky\n",
    "    fo.close()\n",
    "    return(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zero_center_pixels(data):\n",
    "    scale = StandardScaler().fit(data)\n",
    "    old_mean = scale.mean_.astype('float32')\n",
    "    old_sigma = np.sqrt(scale.var_).astype('float32')\n",
    "    return((data - old_mean) * 0.5 / old_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read in the data\n",
    "## manually is easy\n",
    "data_batch_1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "data_batch_2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "data_batch_3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "data_batch_4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "data_batch_5 = unpickle('cifar-10-batches-py/data_batch_5')\n",
    "test_batch = unpickle('cifar-10-batches-py/test_batch')\n",
    "label_map = unpickle('cifar-10-batches-py/batches.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "## stack up the data batches\n",
    "all_data = []\n",
    "\n",
    "for i in data_batch_1['data']:\n",
    "    all_data.append(i)\n",
    "    \n",
    "for i in data_batch_2['data']:\n",
    "    all_data.append(i)\n",
    "\n",
    "for i in data_batch_3['data']:\n",
    "    all_data.append(i)\n",
    "\n",
    "for i in data_batch_4['data']:\n",
    "    all_data.append(i)\n",
    "    \n",
    "for i in data_batch_5['data']:\n",
    "    all_data.append(i)\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "## stack up the label data\n",
    "all_labels = data_batch_1['labels'] + data_batch_2['labels'] + data_batch_3['labels'] + data_batch_4['labels'] + data_batch_5['labels']\n",
    "all_labels = np.array(all_labels)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xx/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/xx/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (50000, 3072) (50000,)\n",
      "Test set (10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "## some of this code was repurposed from Week 7 Section Jupyter Notebook\n",
    "## create the train and test sets\n",
    "## using standard scaler to standardize\n",
    "train_dataset = zero_center_pixels(all_data)\n",
    "train_labels = all_labels\n",
    "#valid_dataset = save['valid_dataset']\n",
    "#valid_labels = save['valid_labels']\n",
    "test_dataset = zero_center_pixels(test_batch['data'])\n",
    "test_labels = np.array(test_batch['labels'])\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "#print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (50000, 32, 32, 3) (50000, 10)\n",
      "Test set (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "##  this code was repurposed from Week 7 Section Jupyter Notebook\n",
    "## reshaping the data to 4D Tensor \n",
    "\n",
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 3 # RGB\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return(dataset, labels)\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "#valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "#print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Step 5 -\n",
    "============\n",
    "Convolutional Neural Nets\n",
    "---------\n",
    "\n",
    "Now we turn to building a CNN using the architecture described in the TensorFlow website tutorial [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html). It will be a small network with two convolutional layers, followed by one fully connected layer. We'll limit the depth of this model so that it will run fine on the CPU and give a more elaborate architecture below that you can experiment with. Most production CNNs run on GPUs because they require more computation power. You can also find this example explained in Chapter 5 of [First Contact with TensorFlow](http://www.jorditorres.org/first-contact-with-tensorflow/) and in Chapter 5 of \"Getting started with TensorFlow\" available on [Proquest through Hollis](http://proquest.safaribooksonline.com.ezp-prod1.hul.harvard.edu/book/programming/machine-learning/9781786468574)\n",
    "\n",
    "### Other useful references\n",
    "There are also web books: Michael A. Nielsen, [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap6.html) and Goodfellow-et-al [Deep Learning](http://www.deeplearningbook.org/) (which is a preprint, available in HTML). There is also [Colah's blog](http://colah.github.io/) that explains convolutions and has some impressive visualizations. \n",
    "\n",
    "If you want to experiment hands-on with convolutions, then download [the GIMP](https://www.gimp.org/downloads/) (if you don't already have it), load up some of your favourite images, and then go to Filters $\\rightarrow$ Generic $\\rightarrow$ Convolution Matrix.  \n",
    "[Here](https://docs.gimp.org/en/plug-in-convmatrix.html) are some examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choices to make in a CNN\n",
    "* convolution kernel dimensions: the patch size (e.g. 5x5)\n",
    "* stride length: 1, 2, something else\n",
    "* the padding, SAME or VALID\n",
    "* pooling: average or max pooling, pooling size  \n",
    "\n",
    "\n",
    "To make the CNN, we will use two other `nn` functions in TensorFlow. The first is for the convolution itself, called [```tf.nn.conv2d()```](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d) . The function takes a 4D input tensor (hence the reformatting in the previous box), the weight variable and then both the stride and padding are specified as parameters.  \n",
    "\n",
    "The second function is the pooling function, either [```tf.nn.max_pool```](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool) or [```tf.nn.avg_pool```](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#avg_pool). These functions take a 4D input tensor, along with the pooling size, the stride and the padding specified as parameters. We will show max_pool in the example.  \n",
    "\n",
    "Note that between the parameters in convolution and the pooling, we have accumulated a number of hyper-parameters - this gives rise to a large number of options for tuning the best performing model, even without considering regularization/dropout and fully connected layers.\n",
    "\n",
    "We redefine the weight and bias functions as per the TensorFlow example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this code was repurposed from Week 7 Section Jupyter Notebook\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this code was repurposed from Week 7 Section Jupyter Notebook\n",
    "\n",
    "# Re-define the function to include the keep probability\n",
    "def run_session(num_epochs, name, k_prob=1.0):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/tensorflowlogs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : k_prob}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                #print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "        print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))\n",
    "        #test_preds[name] = test_prediction.eval().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this code was repurposed from Week 7 Section Jupyter Notebook\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 3 # RGB\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_size // 4 * image_size // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return(tf.matmul(full3, layer4_weights) + layer4_biases)\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    #valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at epoch 0: 2.289539098739624\n",
      "Minibatch accuracy: 18.8\n",
      "Minibatch loss at epoch 500: 1.9600701332092285\n",
      "Minibatch accuracy: 12.5\n",
      "Minibatch loss at epoch 1000: 2.0578835010528564\n",
      "Minibatch accuracy: 18.8\n",
      "Minibatch loss at epoch 1500: 1.1007812023162842\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 2000: 1.6407337188720703\n",
      "Minibatch accuracy: 43.8\n",
      "Minibatch loss at epoch 2500: 1.7023730278015137\n",
      "Minibatch accuracy: 25.0\n",
      "Minibatch loss at epoch 3000: 1.846015453338623\n",
      "Minibatch accuracy: 37.5\n",
      "Minibatch loss at epoch 3500: 1.9532121419906616\n",
      "Minibatch accuracy: 18.8\n",
      "Minibatch loss at epoch 4000: 1.5202109813690186\n",
      "Minibatch accuracy: 43.8\n",
      "Minibatch loss at epoch 4500: 1.606186866760254\n",
      "Minibatch accuracy: 43.8\n",
      "Minibatch loss at epoch 5000: 0.9048736095428467\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 5500: 2.0137503147125244\n",
      "Minibatch accuracy: 25.0\n",
      "Minibatch loss at epoch 6000: 1.177826166152954\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 6500: 1.3649075031280518\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 7000: 1.4036197662353516\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 7500: 1.3093725442886353\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 8000: 1.5706324577331543\n",
      "Minibatch accuracy: 25.0\n",
      "Minibatch loss at epoch 8500: 0.7874034643173218\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 9000: 1.1356945037841797\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 9500: 1.34702730178833\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 10000: 1.218048334121704\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 10500: 1.036126732826233\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 11000: 0.7722863554954529\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 11500: 1.5599359273910522\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 12000: 1.3129305839538574\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 12500: 1.2217967510223389\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 13000: 1.1788828372955322\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 13500: 1.1335327625274658\n",
      "Minibatch accuracy: 43.8\n",
      "Minibatch loss at epoch 14000: 1.3100662231445312\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 14500: 1.412391185760498\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 15000: 1.150437831878662\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 15500: 1.4498507976531982\n",
      "Minibatch accuracy: 43.8\n",
      "Minibatch loss at epoch 16000: 1.2814700603485107\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 16500: 0.9228872060775757\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 17000: 1.1148810386657715\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 17500: 1.1305820941925049\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 18000: 0.9834192395210266\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 18500: 1.3106358051300049\n",
      "Minibatch accuracy: 37.5\n",
      "Minibatch loss at epoch 19000: 1.4466454982757568\n",
      "Minibatch accuracy: 43.8\n",
      "Minibatch loss at epoch 19500: 1.5545141696929932\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 20000: 1.0187931060791016\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 20500: 1.0709155797958374\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 21000: 0.9891473054885864\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 21500: 1.1304831504821777\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 22000: 0.8450298309326172\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 22500: 0.9089714288711548\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 23000: 1.1465328931808472\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 23500: 1.4095369577407837\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 24000: 0.9437987804412842\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 24500: 1.398844599723816\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 25000: 0.7737988829612732\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 25500: 1.1047171354293823\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 26000: 0.689373254776001\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 26500: 1.0106405019760132\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 27000: 0.8732219934463501\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 27500: 1.3719502687454224\n",
      "Minibatch accuracy: 43.8\n",
      "Minibatch loss at epoch 28000: 1.047321081161499\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 28500: 0.4901142120361328\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 29000: 0.9304015040397644\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 29500: 0.820723831653595\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 30000: 1.1599316596984863\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 30500: 0.6945129036903381\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 31000: 0.8463321924209595\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 31500: 0.8583784103393555\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 32000: 0.4176810681819916\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 32500: 0.8531314134597778\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 33000: 0.8311251997947693\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 33500: 1.0655018091201782\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 34000: 0.5359581112861633\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 34500: 0.6786283850669861\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 35000: 0.842156171798706\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 35500: 1.1515846252441406\n",
      "Minibatch accuracy: 50.0\n",
      "Minibatch loss at epoch 36000: 0.9444100260734558\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 36500: 0.761486291885376\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 37000: 0.5634225606918335\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 37500: 0.6643928289413452\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 38000: 0.8217126131057739\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 38500: 0.4333273768424988\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 39000: 0.3777526021003723\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 39500: 0.802132248878479\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 40000: 0.843116283416748\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 40500: 0.37485408782958984\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 41000: 1.0901055335998535\n",
      "Minibatch accuracy: 56.2\n",
      "Minibatch loss at epoch 41500: 0.5908421874046326\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 42000: 0.8465893268585205\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 42500: 0.21764421463012695\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 43000: 0.5865567326545715\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 43500: 0.33407020568847656\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 44000: 0.302673876285553\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 44500: 0.6493943333625793\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 45000: 0.8036438226699829\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 45500: 0.7907142639160156\n",
      "Minibatch accuracy: 62.5\n",
      "Minibatch loss at epoch 46000: 0.6096687316894531\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 46500: 0.9393354654312134\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 47000: 0.33322620391845703\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 47500: 0.6227211356163025\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 48000: 0.43763700127601624\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 48500: 0.6960326433181763\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 49000: 0.26460057497024536\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 49500: 0.3345305919647217\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 50000: 0.2822510004043579\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 50500: 0.4099808931350708\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 51000: 0.46229809522628784\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 51500: 0.784949004650116\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 52000: 0.7149728536605835\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 52500: 1.0011625289916992\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 53000: 0.3165793716907501\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 53500: 0.28814810514450073\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 54000: 0.7638317346572876\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 54500: 0.316272497177124\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 55000: 0.321978896856308\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 55500: 0.6052076816558838\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 56000: 0.38134676218032837\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 56500: 0.16172009706497192\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 57000: 0.3446098566055298\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 57500: 0.4836258590221405\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 58000: 0.2374914139509201\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 58500: 0.7031755447387695\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 59000: 0.22035148739814758\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 59500: 0.4208303391933441\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 60000: 0.5172683000564575\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 60500: 0.2989276349544525\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 61000: 0.11813656240701675\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 61500: 0.6221235990524292\n",
      "Minibatch accuracy: 68.8\n",
      "Minibatch loss at epoch 62000: 0.20155708491802216\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 62500: 0.3302220106124878\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 63000: 0.6238245964050293\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 63500: 0.2866945266723633\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 64000: 0.49930793046951294\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 64500: 0.3817196786403656\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 65000: 0.18570831418037415\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 65500: 0.08223699033260345\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 66000: 0.6956031918525696\n",
      "Minibatch accuracy: 75.0\n",
      "Minibatch loss at epoch 66500: 0.39914190769195557\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 67000: 0.4884801506996155\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 67500: 0.33017218112945557\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 68000: 0.5307889580726624\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 68500: 0.40957120060920715\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 69000: 0.08750566095113754\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 69500: 0.24108301103115082\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 70000: 0.22069062292575836\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 70500: 0.11983068287372589\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 71000: 0.8183233737945557\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 71500: 0.15922531485557556\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 72000: 0.3685897886753082\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 72500: 0.3966059684753418\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 73000: 0.08684081584215164\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 73500: 0.22075149416923523\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 74000: 0.1543312668800354\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 74500: 0.13128703832626343\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 75000: 0.14664550125598907\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 75500: 0.2900823652744293\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 76000: 0.19230559468269348\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 76500: 0.31546998023986816\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 77000: 0.07893872261047363\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 77500: 0.2407015562057495\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 78000: 0.054327838122844696\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 78500: 0.08430565893650055\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 79000: 0.3643304109573364\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 79500: 0.10153162479400635\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 80000: 0.1255093812942505\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 80500: 0.0889858603477478\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 81000: 0.29905965924263\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 81500: 0.08000995963811874\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 82000: 0.49435654282569885\n",
      "Minibatch accuracy: 81.2\n",
      "Minibatch loss at epoch 82500: 0.3556410074234009\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 83000: 0.1857433319091797\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 83500: 0.15936869382858276\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 84000: 0.278199702501297\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 84500: 0.1313537359237671\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 85000: 0.1961355358362198\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 85500: 0.1737709790468216\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 86000: 0.16180051863193512\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 86500: 0.376801997423172\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 87000: 0.08419032394886017\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 87500: 0.21765732765197754\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 88000: 0.24470964074134827\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 88500: 0.35058265924453735\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 89000: 0.14091607928276062\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 89500: 0.16767996549606323\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 90000: 0.10226190835237503\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 90500: 0.2186303734779358\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 91000: 0.19369053840637207\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 91500: 0.4355946481227875\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 92000: 0.024737177416682243\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 92500: 0.05402757227420807\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 93000: 0.030765429139137268\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 93500: 0.09187300503253937\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 94000: 0.5440847277641296\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 94500: 0.15872928500175476\n",
      "Minibatch accuracy: 87.5\n",
      "Minibatch loss at epoch 95000: 0.08646108955144882\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 95500: 0.0886165052652359\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 96000: 0.1895798295736313\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 96500: 0.044039011001586914\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 97000: 0.19924870133399963\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 97500: 0.11322042346000671\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 98000: 0.034179747104644775\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 98500: 0.09894328564405441\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 99000: 0.0895317867398262\n",
      "Minibatch accuracy: 100.0\n",
      "Minibatch loss at epoch 99500: 0.15372948348522186\n",
      "Minibatch accuracy: 93.8\n",
      "Minibatch loss at epoch 100000: 0.38175392150878906\n",
      "Minibatch accuracy: 93.8\n",
      "Test accuracy: 63.8\n",
      "7386.49302983284\n"
     ]
    }
   ],
   "source": [
    "## this code was repurposed from Week 7 Section Jupyter Notebook\n",
    "t0 = time.time()\n",
    "run_session(100001, \"CNN\", 0.5)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
